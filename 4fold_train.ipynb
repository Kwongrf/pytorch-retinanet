{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start importing package...\n",
      "Exception in user code:\n",
      "------------------------------------------------------------\n",
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/connection.py\", line 141, in _new_conn\n",
      "    (self.host, self.port), self.timeout, **extra_kw)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/util/connection.py\", line 83, in create_connection\n",
      "    raise err\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 601, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 357, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/http/client.py\", line 1107, in request\n",
      "    self._send_request(method, url, body, headers)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/http/client.py\", line 1152, in _send_request\n",
      "    self.endheaders(body)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/http/client.py\", line 1103, in endheaders\n",
      "    self._send_output(message_body)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/http/client.py\", line 934, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/http/client.py\", line 877, in send\n",
      "    self.connect()\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/connection.py\", line 166, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/connection.py\", line 150, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e)\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb138d9ed30>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/requests/adapters.py\", line 445, in send\n",
      "    timeout=timeout\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/connectionpool.py\", line 639, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/urllib3/util/retry.py\", line 388, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/retinanet_4fold (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb138d9ed30>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/visdom/__init__.py\", line 446, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/requests/api.py\", line 112, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/requests/api.py\", line 58, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/requests/sessions.py\", line 512, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/requests/sessions.py\", line 622, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/requests/adapters.py\", line 513, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/retinanet_4fold (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb138d9ed30>: Failed to establish a new connection: [Errno 111] Connection refused',))\n"
     ]
    }
   ],
   "source": [
    "# %load retinanet_train.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "print(\"Start importing package...\")\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import pdb\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "\n",
    "import model\n",
    "from anchors import Anchors\n",
    "import losses\n",
    "from dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mydataloader import MyDataset\n",
    "import coco_eval\n",
    "import csv_eval\n",
    "import visdom\n",
    "vis = visdom.Visdom(env='retinanet_4fold')\n",
    "\n",
    "assert torch.__version__.split('.')[1] == '4'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#参数\n",
    "NAME=\"RSNA\"\n",
    "DATA_PATH = \"/data/krf/dataset\"\n",
    "CSV_TRAINS = [DATA_PATH + \"/csv_train0.csv\",DATA_PATH + \"/csv_train1.csv\",DATA_PATH + \"/csv_train2.csv\",DATA_PATH + \"/csv_train3.csv\"]\n",
    "CSV_VALS = [DATA_PATH + \"/csv_val0.csv\",DATA_PATH + \"/csv_val1.csv\",DATA_PATH + \"/csv_val2.csv\",DATA_PATH + \"/csv_val3.csv\"]\n",
    "CSV_CLASSES = DATA_PATH + \"/classes.csv\"\n",
    "DEPTH = 101\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE=2\n",
    "VAL_SIZE = 3000\n",
    "#TRAIN_SIZE = 100\n",
    "#数据预处理\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25684/25684 [01:19<00:00, 324.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0] [2235, 2293, 2226, 2210]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# with open(DATA_PATH+\"/stage_1_train_labels.csv\") as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     rows=[row for row in  reader]\n",
    "#     rows = rows[1:]\n",
    "#     random.shuffle(rows)\n",
    "#     for row in rows:\n",
    "#         row[0] = DATA_PATH+\"/stage_1_train_images/\"+row[0]+\".dcm\"\n",
    "#         if row[1] == '' and row[2] == '' and row[3] == '' and row[4] == '':\n",
    "#             row[5] = ''\n",
    "#         else:\n",
    "#             row[3] = str(float(row[1]) + float(row[3]))# x2 = x1 + w \n",
    "#             row[4] = str(float(row[2]) + float(row[4]))# y2 = y1 + h\n",
    "#     val_rows = rows[:VAL_SIZE]\n",
    "#     train_rows = rows[VAL_SIZE:]\n",
    "#     print(len(val_rows),len(train_rows))\n",
    "#     with open(CSV_TRAIN,'w') as f2:\n",
    "#         write = csv.writer(f2)\n",
    "#         write.writerows(train_rows)\n",
    "#         print(\"csv_train 写入完毕\")\n",
    "#     with open(CSV_VAL,'w') as f3:\n",
    "#         write = csv.writer(f3)\n",
    "#         write.writerows(val_rows)\n",
    "#         print(\"csv_val 写入完毕\")\n",
    "\n",
    "# with open(CSV_CLASSES,'w') as f:\n",
    "#     write = csv.writer(f)\n",
    "#     row = ['1','0']\n",
    "#     write.writerow(row)\n",
    "#     print(\"csv_classes 写入完毕\")\n",
    "\n",
    "\n",
    "\n",
    "#每次跑这个函数之前需要先删除之前的\n",
    "\n",
    "df = pd.read_csv(DATA_PATH+\"/stage_1_train_labels.csv\")\n",
    "train_images = os.listdir(DATA_PATH+\"/stage_1_train_images\")\n",
    "random.shuffle(train_images)#打乱图片顺序\n",
    "count = 0\n",
    "pos_cnt_train = [0,0,0,0]\n",
    "pos_cnt_val = [0,0,0,0]\n",
    "for img_name in tqdm(train_images):\n",
    "    results = df[df['patientId']==img_name.split('.')[0]].values\n",
    "    for row in results:\n",
    "        row[0] = DATA_PATH+\"/stage_1_train_images/\"+row[0]+\".dcm\"\n",
    "        if row[5] == 1:\n",
    "            pos_cnt_val[count % 4] += 1\n",
    "        if row[1] >= 0 and row[1] <= 1024:\n",
    "            row[3] = str(float(row[1]) + float(row[3]))# x2 = x1 + w \n",
    "            row[4] = str(float(row[2]) + float(row[4]))# y2 = y1 + h\n",
    "        else:\n",
    "            row[1] = ''\n",
    "            row[2] = ''\n",
    "            row[3] = ''\n",
    "            row[4] = ''\n",
    "            row[5] = ''\n",
    "        with open(CSV_VALS[count % 4],'a') as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerow(row)\n",
    "        for i in range(4):\n",
    "            if count % 4 == i:\n",
    "                continue\n",
    "            else:\n",
    "                with open(CSV_TRAINS[count % 4],'a') as f:\n",
    "                    write = csv.writer(f)\n",
    "                    write.writerow(row) \n",
    "    count += 1\n",
    "print(pos_cnt_train,pos_cnt_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.87 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "#%time\n",
    "#制作数据loader\n",
    "dataset_train = []\n",
    "dataset_val = []\n",
    "\n",
    "for i in range(4): \n",
    "    dataset_train.append(MyDataset(train_file=CSV_TRAINS[i], class_list=CSV_CLASSES, transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()])))\n",
    "    dataset_val.append(MyDataset(train_file=CSV_VALS[i], class_list=CSV_CLASSES, transform=transforms.Compose([Normalizer(), Resizer()])))\n",
    "\n",
    "#每次的sampler的参数：来源、batchsize、是否抛弃最后一层？？？\n",
    "\n",
    "# num_workers 同时工作的组？collater:校验用的吧\n",
    "dataloader_train = []\n",
    "dataloader_val = []\n",
    "\n",
    "for i in range(4):\n",
    "    sampler = AspectRatioBasedSampler(dataset_train[i], batch_size=BATCH_SIZE, drop_last=False)\n",
    "    sampler_val = AspectRatioBasedSampler(dataset_val[i], batch_size=1, drop_last=False)\n",
    "    dataloader_train.append(DataLoader(dataset_train[i], num_workers=1, collate_fn=collater, batch_sampler=sampler))\n",
    "    dataloader_val.append(DataLoader(dataset_val[i], num_workers=1, collate_fn=collater, batch_sampler=sampler_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training images: 5421\n",
      "Num training images: 5421\n",
      "Num training images: 5421\n",
      "Num training images: 5421\n"
     ]
    }
   ],
   "source": [
    "retinanets = []\n",
    "# Create the model\n",
    "if DEPTH == 18:\n",
    "    retinanet = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif DEPTH == 34:\n",
    "    retinanet = model.resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif DEPTH == 50:\n",
    "    for i in range(4):\n",
    "        retinanets.append(model.resnet50(num_classes=dataset_train[i].num_classes(), pretrained=True))\n",
    "elif DEPTH == 101:\n",
    "    for i in range(4):\n",
    "        retinanets.append(model.resnet101(num_classes=dataset_train[i].num_classes(), pretrained=True)) \n",
    "elif DEPTH == 152:\n",
    "    for i in range(4):\n",
    "        retinanets.append(model.resnet152(num_classes=dataset_train[i].num_classes(), pretrained=True))\n",
    "else:\n",
    "    raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')\n",
    "#retinanet = torch.load('weights/RSNA_retinanet_5.pt')\n",
    "\n",
    "optimizer = []\n",
    "scheduler = []\n",
    "loss_hist = []\n",
    "for i in range(4):\n",
    "    retinanets[i] = retinanets[i].cuda()\n",
    "    #变成并行\n",
    "    retinanets[i] = torch.nn.DataParallel(retinanets[i]).cuda()\n",
    "    #训练模式\n",
    "    retinanets[i].training = True\n",
    "    #学习率0.00001  \n",
    "    optimizer.append(optim.Adam(retinanets[i].parameters(), lr=1e-5))\n",
    "    #如果3个epoch损失没有减少则降低学习率\n",
    "    scheduler.append(optim.lr_scheduler.ReduceLROnPlateau(optimizer[i], patience=2, verbose=True))\n",
    "    # TODO 这是干什么 deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：这里是定义了一个500的队列\n",
    "    loss_hist.append(collections.deque(maxlen=1000))\n",
    "    print('Num training images: {}'.format(len(dataset_train[i])))\n",
    "# In[5]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iteration: 2 | Classification loss: 0.30485 | Regression loss: 0.45600 | Running loss: 1.02187\n",
      "Epoch: 0 | Iteration: 5 | Classification loss: 0.47479 | Regression loss: 0.60512 | Running loss: 1.02463\n",
      "Epoch: 0 | Iteration: 7 | Classification loss: 0.33693 | Regression loss: 0.51016 | Running loss: 1.01656\n",
      "Epoch: 0 | Iteration: 10 | Classification loss: 0.24431 | Regression loss: 0.52867 | Running loss: 1.00597\n",
      "Epoch: 0 | Iteration: 13 | Classification loss: 0.34512 | Regression loss: 0.61866 | Running loss: 1.00421\n",
      "Epoch: 0 | Iteration: 15 | Classification loss: 0.26517 | Regression loss: 0.45770 | Running loss: 0.99296\n",
      "Epoch: 0 | Iteration: 16 | Classification loss: 0.35947 | Regression loss: 0.48041 | Running loss: 0.98707\n",
      "Epoch: 0 | Iteration: 18 | Classification loss: 0.29362 | Regression loss: 0.48059 | Running loss: 0.97919\n",
      "Epoch: 0 | Iteration: 22 | Classification loss: 0.30212 | Regression loss: 0.62554 | Running loss: 0.97735\n",
      "Epoch: 0 | Iteration: 24 | Classification loss: 0.31241 | Regression loss: 0.44827 | Running loss: 0.96988\n",
      "Epoch: 0 | Iteration: 29 | Classification loss: 0.57720 | Regression loss: 0.92048 | Running loss: 0.98747\n",
      "Epoch: 0 | Iteration: 30 | Classification loss: 0.22314 | Regression loss: 0.47801 | Running loss: 0.97823\n",
      "Epoch: 0 | Iteration: 33 | Classification loss: 0.62014 | Regression loss: 0.94419 | Running loss: 0.99655\n",
      "Epoch: 0 | Iteration: 35 | Classification loss: 0.23259 | Regression loss: 0.45868 | Running loss: 0.98730\n",
      "Epoch: 0 | Iteration: 36 | Classification loss: 0.18244 | Regression loss: 0.48632 | Running loss: 0.97793\n",
      "Epoch: 0 | Iteration: 37 | Classification loss: 0.40304 | Regression loss: 0.49038 | Running loss: 0.97552\n",
      "Epoch: 0 | Iteration: 39 | Classification loss: 0.32461 | Regression loss: 0.47752 | Running loss: 0.97070\n",
      "Epoch: 0 | Iteration: 48 | Classification loss: 0.73867 | Regression loss: 1.02157 | Running loss: 0.99204\n",
      "Epoch: 0 | Iteration: 49 | Classification loss: 0.19406 | Regression loss: 0.42693 | Running loss: 0.98227\n",
      "Epoch: 0 | Iteration: 50 | Classification loss: 0.42308 | Regression loss: 0.95978 | Running loss: 0.99255\n",
      "Epoch: 0 | Iteration: 51 | Classification loss: 0.36402 | Regression loss: 0.54713 | Running loss: 0.99051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5e448191219f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0miter_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader timed out after {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/data/krf/model/pytorch-retinanet/dataloader.py\", line 387, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/data/krf/model/pytorch-retinanet/dataloader.py\", line 595, in __call__\n",
      "    return {'img':((image.astype(np.float32)-self.mean)/self.std), 'annot': annots}\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "count = [0,0,0,0]\n",
    "import traceback\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "    for i in range(4):\n",
    "        retinanets[i].train()\n",
    "        retinanets[i].module.freeze_bn()\n",
    "        epoch_loss = []\n",
    "        \n",
    "        for iter_num, data in enumerate(dataloader_train[i]):\n",
    "            try:\n",
    "                optimizer[i].zero_grad()\n",
    "\n",
    "                classification_loss, regression_loss = retinanets[i]([Variable(data['img'].cuda().float()), Variable(data['annot'].cuda())])\n",
    "                classification_loss = classification_loss.mean()\n",
    "                regression_loss = regression_loss.mean()\n",
    "\n",
    "                loss = classification_loss + regression_loss\n",
    "\n",
    "                if bool(loss == 0):\n",
    "                    continue\n",
    "                #反向传播？\n",
    "                loss.backward()\n",
    "\n",
    "                #这是干嘛？？TODO\n",
    "                torch.nn.utils.clip_grad_norm_(retinanets[i].parameters(), 0.1)\n",
    "\n",
    "                #这?TODO\n",
    "                optimizer[i].step()\n",
    "\n",
    "                loss_hist[i].append(float(loss))\n",
    "\n",
    "                epoch_loss.append(float(loss))\n",
    "\n",
    "                print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist[i])))\n",
    "                vis.line(X=torch.Tensor([count[i]]), Y=torch.Tensor([np.mean(loss_hist[i])]), win='train loss '+str(i), update='append' ,opts={'title':'train loss '+str(i)})\n",
    "                count[i] += 1\n",
    "                vis.save(['retinanet_4fold'])\n",
    "\n",
    "                del classification_loss\n",
    "                del regression_loss\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "        print(\"Evaluating dataset\")\n",
    "    #     retinanet.eval()\n",
    "    #     for index,data in enumerate(dataloader_val):\n",
    "    #         #data = dataset[index]\n",
    "    #         scale = data['scale']\n",
    "    #         # run network\n",
    "    #         #scores, labels, boxes = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "    #         print(Variable(data['img'].cuda()))\n",
    "    #         scores, labels, boxes = retinanet(Variable(data['img'].cuda()))\n",
    "    #         #mAP = csv_eval.evaluate(dataset_val,retinanet)\n",
    "        try:\n",
    "            mAP = csv_eval.evaluate(dataset_val[i],retinanets[i])\n",
    "            vis.line(X=torch.Tensor([epoch_num]), Y=torch.Tensor([mAP[0][0]]), win='val mAP '+str(i), update='append' ,opts={'title':'mAP val '+str(i)})\n",
    "            vis.save(['retinanet_4fold'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print( 'traceback.print_exc():', traceback.print_exc())\n",
    "            continue\n",
    "        #这一步也看不懂？？TODO \n",
    "        scheduler[i].step(np.mean(epoch_loss))\n",
    "\n",
    "        torch.save(retinanets[i].module, 'weights_stage1/{}_retinanet_{}.pt'.format(i, epoch_num))\n",
    "    #     torch.save(retinanet.module, '{}_retinanet_{}.pt'.format(parser.dataset, epoch_num))\n",
    "\n",
    "for i in range(4):\n",
    "    retinanets[i].eval()\n",
    "    torch.save(retinanets[i], 'weights_stage1/{}_model_final.pt'.format(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###集成四个模型\n",
    "'''\n",
    "models 是四个模型权重目录列表\n",
    "'''\n",
    "def essemble(models):\n",
    "    for i in range(4):\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
