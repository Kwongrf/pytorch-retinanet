{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import pdb\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "\n",
    "import pydicom\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import skimage.color\n",
    "import skimage\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import model\n",
    "from anchors import Anchors\n",
    "import losses\n",
    "from dataloader import CocoDataset, CSVDataset, MyDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import coco_eval\n",
    "import csv_eval\n",
    "\n",
    "assert torch.__version__.split('.')[1] == '4'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#参数\n",
    "NAME=\"RSNA\"\n",
    "DATA_PATH = \"/data/krf/dataset\"\n",
    "CSV_TRAIN = DATA_PATH + \"/csv_train.csv\"\n",
    "CSV_VAL = DATA_PATH + \"/csv_val.csv\"\n",
    "CSV_CLASSES = DATA_PATH + \"/classes.csv\"\n",
    "TEST_PATH = DATA_PATH +\"/stage_2_test_images\"\n",
    "DEPTH = 50\n",
    "EPOCHS = 10\n",
    "\n",
    "VAL_SIZE = 10\n",
    "TRAIN_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 39\n",
      "csv_train 写入完毕\n",
      "csv_val 写入完毕\n",
      "csv_classes 写入完毕\n"
     ]
    }
   ],
   "source": [
    "#数据预处理\n",
    "import csv\n",
    "import random\n",
    "with open(DATA_PATH+\"/stage_2_train_labels.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    rows=[row for row in  reader]\n",
    "    rows = rows[1:TRAIN_SIZE]\n",
    "    random.shuffle(rows)\n",
    "    for row in rows:\n",
    "        row[0] = DATA_PATH+\"/stage_2_train_images/\"+row[0]+\".dcm\"\n",
    "        if row[1] == '' and row[2] == '' and row[3] == '' and row[4] == '':\n",
    "            row[5] = ''\n",
    "        else:\n",
    "            row[3] = str(float(row[1]) + float(row[3]))# x2 = x1 + w \n",
    "            row[4] = str(float(row[2]) + float(row[4]))# y2 = y1 + h\n",
    "    val_rows = rows[:VAL_SIZE]\n",
    "    train_rows = rows[VAL_SIZE:]\n",
    "    print(len(val_rows),len(train_rows))\n",
    "    with open(CSV_TRAIN,'w') as f2:\n",
    "        write = csv.writer(f2)\n",
    "        write.writerows(train_rows)\n",
    "        print(\"csv_train 写入完毕\")\n",
    "    with open(CSV_VAL,'w') as f3:\n",
    "        write = csv.writer(f3)\n",
    "        write.writerows(val_rows)\n",
    "        print(\"csv_val 写入完毕\")\n",
    "\n",
    "with open(CSV_CLASSES,'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    row = ['1','0']\n",
    "    write.writerow(row)\n",
    "    print(\"csv_classes 写入完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time\n",
    "#制作数据loader\n",
    "dataset_train = MyDataset(train_file=CSV_TRAIN, class_list=CSV_CLASSES, transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "dataset_val = MyDataset(train_file=CSV_VAL, class_list=CSV_CLASSES, transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "#每次的sampler的参数：来源、batchsize、是否抛弃最后一层？？？\n",
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=2, drop_last=False)\n",
    "# num_workers 同时工作的组？collater:校验用的吧\n",
    "dataloader_train = DataLoader(dataset_train, num_workers=1, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "# if dataset_val is not None:\n",
    "#     sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "#     dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, num_workers=1, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "if DEPTH == 18:\n",
    "    retinanet = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif DEPTH == 34:\n",
    "    retinanet = model.resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif DEPTH == 50:\n",
    "    retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif DEPTH == 101:\n",
    "    retinanet = model.resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif DEPTH == 152:\n",
    "    retinanet = model.resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "else:\n",
    "    raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "if use_gpu:\n",
    "    retinanet = retinanet.cuda()\n",
    "#变成并行\n",
    "retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "#训练模式\n",
    "retinanet.training = True\n",
    "#学习率0.00001  \n",
    "optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "#如果3个epoch损失没有减少则降低学习率\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "# TODO 这是干什么 deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：这里是定义了一个500的队列\n",
    "loss_hist = collections.deque(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training images: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iteration: 3 | Classification loss: 0.56482 | Regression loss: 0.50520 | Running loss: 1.07002\n",
      "Epoch: 0 | Iteration: 5 | Classification loss: 1.13002 | Regression loss: 1.04749 | Running loss: 1.62377\n",
      "Epoch: 0 | Iteration: 6 | Classification loss: 1.10966 | Regression loss: 0.95433 | Running loss: 1.77051\n",
      "Epoch: 0 | Iteration: 7 | Classification loss: 0.55578 | Regression loss: 0.57191 | Running loss: 1.60980\n",
      "Epoch: 0 | Iteration: 8 | Classification loss: 0.54173 | Regression loss: 0.59898 | Running loss: 1.51599\n",
      "Epoch: 0 | Iteration: 9 | Classification loss: 0.53452 | Regression loss: 0.54088 | Running loss: 1.44256\n",
      "Epoch: 0 | Iteration: 11 | Classification loss: 0.51360 | Regression loss: 0.54176 | Running loss: 1.38724\n",
      "Epoch: 0 | Iteration: 12 | Classification loss: 0.48279 | Regression loss: 0.61190 | Running loss: 1.35067\n",
      "Epoch: 0 | Iteration: 14 | Classification loss: 0.45462 | Regression loss: 0.53646 | Running loss: 1.31072\n",
      "Epoch: 0 | Iteration: 16 | Classification loss: 0.40908 | Regression loss: 0.52096 | Running loss: 1.27265\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.006374359886467319\n",
      "Epoch: 1 | Iteration: 1 | Classification loss: 0.35971 | Regression loss: 0.50813 | Running loss: 1.23585\n",
      "Epoch: 1 | Iteration: 2 | Classification loss: 0.39496 | Regression loss: 0.44572 | Running loss: 1.20292\n",
      "Epoch: 1 | Iteration: 3 | Classification loss: 0.31392 | Regression loss: 0.49643 | Running loss: 1.17272\n",
      "Epoch: 1 | Iteration: 4 | Classification loss: 1.61933 | Regression loss: 0.92635 | Running loss: 1.27079\n",
      "Epoch: 1 | Iteration: 5 | Classification loss: 0.75120 | Regression loss: 0.45613 | Running loss: 1.26656\n",
      "Epoch: 1 | Iteration: 6 | Classification loss: 0.39765 | Regression loss: 0.46079 | Running loss: 1.24105\n",
      "Epoch: 1 | Iteration: 10 | Classification loss: 0.64570 | Regression loss: 0.48547 | Running loss: 1.23459\n",
      "Epoch: 1 | Iteration: 11 | Classification loss: 0.44709 | Regression loss: 0.52278 | Running loss: 1.21988\n",
      "Epoch: 1 | Iteration: 13 | Classification loss: 0.77730 | Regression loss: 1.11294 | Running loss: 1.25516\n",
      "Epoch: 1 | Iteration: 14 | Classification loss: 0.32275 | Regression loss: 0.50690 | Running loss: 1.23389\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.012935141681996627\n",
      "Epoch: 2 | Iteration: 0 | Classification loss: 0.34315 | Regression loss: 0.45444 | Running loss: 1.21311\n",
      "Epoch: 2 | Iteration: 1 | Classification loss: 0.26890 | Regression loss: 0.43394 | Running loss: 1.18992\n",
      "Epoch: 2 | Iteration: 3 | Classification loss: 0.27576 | Regression loss: 0.47616 | Running loss: 1.17087\n",
      "Epoch: 2 | Iteration: 5 | Classification loss: 0.26556 | Regression loss: 0.46938 | Running loss: 1.15271\n",
      "Epoch: 2 | Iteration: 6 | Classification loss: 0.24887 | Regression loss: 0.51094 | Running loss: 1.13699\n",
      "Epoch: 2 | Iteration: 7 | Classification loss: 0.21521 | Regression loss: 0.38596 | Running loss: 1.11639\n",
      "Epoch: 2 | Iteration: 9 | Classification loss: 0.20802 | Regression loss: 0.36962 | Running loss: 1.09643\n",
      "Epoch: 2 | Iteration: 10 | Classification loss: 0.36212 | Regression loss: 0.52374 | Running loss: 1.08891\n",
      "Epoch: 2 | Iteration: 14 | Classification loss: 0.52521 | Regression loss: 1.01205 | Running loss: 1.10437\n",
      "Epoch: 2 | Iteration: 15 | Classification loss: 0.61819 | Regression loss: 1.13488 | Running loss: 1.12600\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.0068268442128624485\n",
      "Epoch: 3 | Iteration: 2 | Classification loss: 0.27773 | Regression loss: 0.50627 | Running loss: 1.11496\n",
      "Epoch: 3 | Iteration: 5 | Classification loss: 0.19484 | Regression loss: 0.35132 | Running loss: 1.09719\n",
      "Epoch: 3 | Iteration: 6 | Classification loss: 0.53579 | Regression loss: 0.95694 | Running loss: 1.10917\n",
      "Epoch: 3 | Iteration: 7 | Classification loss: 0.55171 | Regression loss: 1.07854 | Running loss: 1.12450\n",
      "Epoch: 3 | Iteration: 9 | Classification loss: 0.27269 | Regression loss: 0.47087 | Running loss: 1.11362\n",
      "Epoch: 3 | Iteration: 10 | Classification loss: 0.21371 | Regression loss: 0.43794 | Running loss: 1.10078\n",
      "Epoch: 3 | Iteration: 12 | Classification loss: 0.24721 | Regression loss: 0.39878 | Running loss: 1.08849\n",
      "Epoch: 3 | Iteration: 13 | Classification loss: 0.16277 | Regression loss: 0.35337 | Running loss: 1.07343\n",
      "Epoch: 3 | Iteration: 14 | Classification loss: 0.26682 | Regression loss: 0.42663 | Running loss: 1.06369\n",
      "Epoch: 3 | Iteration: 16 | Classification loss: 0.17576 | Regression loss: 0.36829 | Running loss: 1.05070\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.0203152152378623\n",
      "Epoch: 4 | Iteration: 0 | Classification loss: 0.39720 | Regression loss: 0.90226 | Running loss: 1.05676\n",
      "Epoch: 4 | Iteration: 2 | Classification loss: 0.23826 | Regression loss: 0.41296 | Running loss: 1.04711\n",
      "Epoch: 4 | Iteration: 6 | Classification loss: 0.25333 | Regression loss: 0.46084 | Running loss: 1.03937\n",
      "Epoch: 4 | Iteration: 7 | Classification loss: 0.19215 | Regression loss: 0.35401 | Running loss: 1.02816\n",
      "Epoch: 4 | Iteration: 9 | Classification loss: 0.57506 | Regression loss: 1.04288 | Running loss: 1.04126\n",
      "Epoch: 4 | Iteration: 11 | Classification loss: 0.17428 | Regression loss: 0.41703 | Running loss: 1.03148\n",
      "Epoch: 4 | Iteration: 13 | Classification loss: 0.19864 | Regression loss: 0.43496 | Running loss: 1.02302\n",
      "Epoch: 4 | Iteration: 14 | Classification loss: 0.14115 | Regression loss: 0.34415 | Running loss: 1.01181\n",
      "Epoch: 4 | Iteration: 15 | Classification loss: 0.12741 | Regression loss: 0.34832 | Running loss: 1.00087\n",
      "Epoch: 4 | Iteration: 16 | Classification loss: 0.12300 | Regression loss: 0.33620 | Running loss: 0.99004\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.16059831376137604\n",
      "Epoch: 5 | Iteration: 0 | Classification loss: 0.19075 | Regression loss: 0.40191 | Running loss: 0.98225\n",
      "Epoch: 5 | Iteration: 3 | Classification loss: 0.44392 | Regression loss: 1.00811 | Running loss: 0.99128\n",
      "Epoch: 5 | Iteration: 4 | Classification loss: 0.16920 | Regression loss: 0.41907 | Running loss: 0.98368\n",
      "Epoch: 5 | Iteration: 6 | Classification loss: 0.15795 | Regression loss: 0.39681 | Running loss: 0.97573\n",
      "Epoch: 5 | Iteration: 7 | Classification loss: 0.12634 | Regression loss: 0.33380 | Running loss: 0.96636\n",
      "Epoch: 5 | Iteration: 8 | Classification loss: 0.21854 | Regression loss: 0.42805 | Running loss: 0.96065\n",
      "Epoch: 5 | Iteration: 9 | Classification loss: 0.38428 | Regression loss: 0.88160 | Running loss: 0.96600\n",
      "Epoch: 5 | Iteration: 11 | Classification loss: 0.11210 | Regression loss: 0.33883 | Running loss: 0.95712\n",
      "Epoch: 5 | Iteration: 12 | Classification loss: 0.20567 | Regression loss: 0.44806 | Running loss: 0.95198\n",
      "Epoch: 5 | Iteration: 14 | Classification loss: 0.15011 | Regression loss: 0.30492 | Running loss: 0.94370\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.16168322119100226\n",
      "Epoch: 6 | Iteration: 0 | Classification loss: 0.14650 | Regression loss: 0.37405 | Running loss: 0.93676\n",
      "Epoch: 6 | Iteration: 1 | Classification loss: 0.29715 | Regression loss: 0.87642 | Running loss: 0.94058\n",
      "Epoch: 6 | Iteration: 2 | Classification loss: 0.38417 | Regression loss: 1.00142 | Running loss: 0.94765\n",
      "Epoch: 6 | Iteration: 3 | Classification loss: 0.18651 | Regression loss: 0.42361 | Running loss: 0.94237\n",
      "Epoch: 6 | Iteration: 5 | Classification loss: 0.10560 | Regression loss: 0.29655 | Running loss: 0.93406\n",
      "Epoch: 6 | Iteration: 10 | Classification loss: 0.16546 | Regression loss: 0.39011 | Running loss: 0.92833\n",
      "Epoch: 6 | Iteration: 11 | Classification loss: 0.18826 | Regression loss: 0.33890 | Running loss: 0.92234\n",
      "Epoch: 6 | Iteration: 13 | Classification loss: 0.19101 | Regression loss: 0.35738 | Running loss: 0.91684\n",
      "Epoch: 6 | Iteration: 15 | Classification loss: 0.13302 | Regression loss: 0.35786 | Running loss: 0.91067\n",
      "Epoch: 6 | Iteration: 16 | Classification loss: 0.10183 | Regression loss: 0.31538 | Running loss: 0.90362\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.10372453397268709\n",
      "Epoch: 7 | Iteration: 0 | Classification loss: 0.09494 | Regression loss: 0.31192 | Running loss: 0.89662\n",
      "Epoch: 7 | Iteration: 2 | Classification loss: 0.32000 | Regression loss: 0.96377 | Running loss: 0.90200\n",
      "Epoch: 7 | Iteration: 3 | Classification loss: 0.20742 | Regression loss: 0.27634 | Running loss: 0.89627\n",
      "Epoch: 7 | Iteration: 5 | Classification loss: 0.18071 | Regression loss: 0.34541 | Running loss: 0.89127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Iteration: 8 | Classification loss: 0.12627 | Regression loss: 0.37729 | Running loss: 0.88610\n",
      "Epoch: 7 | Iteration: 10 | Classification loss: 0.14094 | Regression loss: 0.34793 | Running loss: 0.88087\n",
      "Epoch: 7 | Iteration: 13 | Classification loss: 0.28328 | Regression loss: 0.79456 | Running loss: 0.88343\n",
      "Epoch: 7 | Iteration: 14 | Classification loss: 0.07839 | Regression loss: 0.27997 | Running loss: 0.87670\n",
      "Epoch: 7 | Iteration: 15 | Classification loss: 0.18154 | Regression loss: 0.40905 | Running loss: 0.87307\n",
      "Epoch: 7 | Iteration: 16 | Classification loss: 0.14881 | Regression loss: 0.39160 | Running loss: 0.86892\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.11175720955456342\n",
      "Epoch: 8 | Iteration: 0 | Classification loss: 0.25008 | Regression loss: 0.76827 | Running loss: 0.87076\n",
      "Epoch: 8 | Iteration: 1 | Classification loss: 0.06878 | Regression loss: 0.25614 | Running loss: 0.86410\n",
      "Epoch: 8 | Iteration: 2 | Classification loss: 0.15115 | Regression loss: 0.31934 | Running loss: 0.85936\n",
      "Epoch: 8 | Iteration: 3 | Classification loss: 0.08847 | Regression loss: 0.29866 | Running loss: 0.85374\n",
      "Epoch: 8 | Iteration: 4 | Classification loss: 0.12724 | Regression loss: 0.36206 | Running loss: 0.84945\n",
      "Epoch: 8 | Iteration: 6 | Classification loss: 0.49383 | Regression loss: 0.96922 | Running loss: 0.85659\n",
      "Epoch: 8 | Iteration: 7 | Classification loss: 0.14309 | Regression loss: 0.35571 | Running loss: 0.85248\n",
      "Epoch: 8 | Iteration: 10 | Classification loss: 0.06832 | Regression loss: 0.22466 | Running loss: 0.84612\n",
      "Epoch: 8 | Iteration: 13 | Classification loss: 0.07821 | Regression loss: 0.28834 | Running loss: 0.84073\n",
      "Epoch: 8 | Iteration: 16 | Classification loss: 0.24067 | Regression loss: 0.41447 | Running loss: 0.83867\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.023965522959954615\n",
      "Epoch: 9 | Iteration: 0 | Classification loss: 0.19812 | Regression loss: 0.34133 | Running loss: 0.83538\n",
      "Epoch: 9 | Iteration: 2 | Classification loss: 0.12194 | Regression loss: 0.32879 | Running loss: 0.83120\n",
      "Epoch: 9 | Iteration: 7 | Classification loss: 0.14223 | Regression loss: 0.35629 | Running loss: 0.82762\n",
      "Epoch: 9 | Iteration: 8 | Classification loss: 0.08228 | Regression loss: 0.26128 | Running loss: 0.82247\n",
      "Epoch: 9 | Iteration: 9 | Classification loss: 0.31802 | Regression loss: 0.89396 | Running loss: 0.82657\n",
      "Epoch: 9 | Iteration: 10 | Classification loss: 0.22288 | Regression loss: 0.72029 | Running loss: 0.82779\n",
      "Epoch: 9 | Iteration: 12 | Classification loss: 0.07579 | Regression loss: 0.23388 | Running loss: 0.82244\n",
      "Epoch: 9 | Iteration: 13 | Classification loss: 0.13329 | Regression loss: 0.33689 | Running loss: 0.81885\n",
      "Epoch: 9 | Iteration: 14 | Classification loss: 0.12364 | Regression loss: 0.30382 | Running loss: 0.81490\n",
      "Epoch: 9 | Iteration: 15 | Classification loss: 0.08018 | Regression loss: 0.20370 | Running loss: 0.80959\n",
      "Evaluating dataset\n",
      "9/9\n",
      "mAP:\n",
      "1: 0.03775673414935064\n"
     ]
    }
   ],
   "source": [
    "retinanet.train()\n",
    "#BN层冻结！！\n",
    "retinanet.module.freeze_bn()\n",
    "\n",
    "print('Num training images: {}'.format(len(dataset_train)))\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "    retinanet.train()\n",
    "    retinanet.module.freeze_bn()\n",
    "    \n",
    "    epoch_loss = []\n",
    "    \n",
    "    for iter_num, data in enumerate(dataloader_train):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_loss, regression_loss = retinanet([Variable(data['img'].cuda().float()), Variable(data['annot'].cuda())])\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            loss = classification_loss + regression_loss\n",
    "            \n",
    "            if bool(loss == 0):\n",
    "                continue\n",
    "            #反向传播？\n",
    "            loss.backward()\n",
    "\n",
    "            #这是干嘛？？TODO\n",
    "            torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "            #这?TODO\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_hist.append(float(loss))\n",
    "\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "            print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "            del classification_loss\n",
    "            del regression_loss\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "\n",
    "    print(\"Evaluating dataset\")\n",
    "#     retinanet.eval()\n",
    "#     for index,data in enumerate(dataloader_val):\n",
    "#         #data = dataset[index]\n",
    "#         scale = data['scale']\n",
    "#         # run network\n",
    "#         #scores, labels, boxes = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "#         print(Variable(data['img'].cuda()))\n",
    "#         scores, labels, boxes = retinanet(Variable(data['img'].cuda()))\n",
    "#         #mAP = csv_eval.evaluate(dataset_val,retinanet)\n",
    "    mAP = csv_eval.evaluate(dataset_val,retinanet)\n",
    "    #这一步也看不懂？？TODO \n",
    "    scheduler.step(np.mean(epoch_loss))\n",
    "    \n",
    "    torch.save(retinanet.module, 'weights/{}_retinanet_{}.pt'.format(NAME, epoch_num))\n",
    "#     torch.save(retinanet.module, '{}_retinanet_{}.pt'.format(parser.dataset, epoch_num))\n",
    "\n",
    "retinanet.eval()\n",
    "\n",
    "torch.save(retinanet, 'weights/model_final.pt'.format(epoch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-72ce177711e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretinanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights/RSNA_retinanet_3.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mretinanet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/krf/anaconda/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'copy'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################KRF CREATED 2018/11/15###################################################\n",
    "class TestDataset(Dataset):\n",
    "    \"\"\"Test dataset.类似于MyDataset，不过没有csv和label\"\"\"\n",
    "\n",
    "    def __init__(self,test_fp, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            test_fp (string):训练集的文件目录\n",
    "            \n",
    "        \"\"\"\n",
    "        self.test_fp = test_fp\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(self.test_fp)[:100] #只测试100张\n",
    "        for i in range(len(self.image_names)):\n",
    "            self.image_names[i] =  os.path.join(TEST_PATH,self.image_names[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.load_image(idx)\n",
    "        sample = {'img': img,'name' : self.image_names[idx]}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "#         img = skimage.io.imread(self.image_names[image_index])\n",
    "        ds = pydicom.read_file(self.image_names[image_index])\n",
    "        img = ds.pixel_array\n",
    "        \n",
    "        if len(img.shape) == 2:\n",
    "            img = skimage.color.gray2rgb(img)\n",
    "\n",
    "        return img.astype(np.float32)/255.0\n",
    "    \n",
    "    #####################################Modified by KRF######################\n",
    "    def image_aspect_ratio(self, image_index):\n",
    "        #image = Image.open(self.image_names[image_index])\n",
    "        ds = pydicom.read_file(self.image_names[image_index])\n",
    "        img_arr = ds.pixel_array\n",
    "        image = Image.fromarray(img_arr).convert('RGB')\n",
    "        return float(image.width) / float(image.height)\n",
    "###################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#覆盖另一个\n",
    "class NormalizerTest(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean = np.array([[[0.485, 0.456, 0.406]]])\n",
    "        self.std = np.array([[[0.229, 0.224, 0.225]]])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        image, name = sample['img'], sample['name']\n",
    "\n",
    "        return {'img':((image.astype(np.float32)-self.mean)/self.std), 'name': name}\n",
    "class ResizerTest(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    #def __call__(self, sample, min_side=608, max_side=1024):\n",
    "    ###########################################KRF Modeified###########################################################\n",
    "    def __call__(self, sample, min_side=512, max_side=1024):\n",
    "        image, name = sample['img'], sample['name']\n",
    "\n",
    "        rows, cols, cns = image.shape\n",
    "\n",
    "        smallest_side = min(rows, cols)\n",
    "\n",
    "        # rescale the image so the smallest side is min_side\n",
    "        scale = min_side / smallest_side\n",
    "\n",
    "        # check if the largest side is now greater than max_side, which can happen\n",
    "        # when images have a large aspect ratio\n",
    "        largest_side = max(rows, cols)\n",
    "\n",
    "        if largest_side * scale > max_side:\n",
    "            scale = max_side / largest_side\n",
    "\n",
    "        # resize the image with the computed scale\n",
    "        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))\n",
    "        rows, cols, cns = image.shape\n",
    "\n",
    "        pad_w = 32 - rows%32\n",
    "        pad_h = 32 - cols%32\n",
    "\n",
    "        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n",
    "        new_image[:rows, :cols, :] = image.astype(np.float32)\n",
    "\n",
    "        #annots[:, :4] *= scale\n",
    "\n",
    "        return {'img': torch.from_numpy(new_image), 'name': name, 'scale': scale}\n",
    "\n",
    "dataset_test = TestDataset(TEST_PATH,transform=transforms.Compose([NormalizerTest(), ResizerTest()]))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect(dataset, retinanet, score_threshold=0.05, max_detections=100, save_path=None):\n",
    "#     \"\"\" Get the detections from the retinanet using the generator.\n",
    "#     The result is a list of lists such that the size is:\n",
    "#         all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n",
    "#     # Arguments\n",
    "#         dataset         : The generator used to run images through the retinanet.\n",
    "#         retinanet           : The retinanet to run on the images.\n",
    "#         score_threshold : The score confidence threshold to use.\n",
    "#         max_detections  : The maximum number of detections to use per image.\n",
    "#         save_path       : The path to save the images with visualized detections to.\n",
    "#     # Returns\n",
    "#         A list of lists containing the detections for each image in the generator.\n",
    "#     \"\"\"\n",
    "#     all_detections = [[None for i in range(dataset.num_classes())] for j in range(len(dataset))]\n",
    "    \n",
    "#     retinanet.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for index in range(len(dataset)):\n",
    "        \n",
    "#             data = dataset[index]\n",
    "#             scale = data['scale']\n",
    "\n",
    "#             # run network\n",
    "#             scores, labels, boxes = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n",
    "#             #scores, labels, boxes = retinanet(data['img'].cuda().float())\n",
    "#             scores = scores.cpu().numpy()\n",
    "#             labels = labels.cpu().numpy()\n",
    "#             boxes  = boxes.cpu().numpy()\n",
    "\n",
    "#             # correct boxes for image scale\n",
    "#             boxes /= scale\n",
    "\n",
    "#             # select indices which have a score above the threshold\n",
    "#             indices = np.where(scores > score_threshold)[0]\n",
    "#             if indices.shape[0] > 0:\n",
    "#                 # select those scores\n",
    "#                 scores = scores[indices]\n",
    "\n",
    "#                 # find the order with which to sort the scores\n",
    "#                 scores_sort = np.argsort(-scores)[:max_detections]\n",
    "\n",
    "#                 # select detections\n",
    "#                 image_boxes      = boxes[indices[scores_sort], :]\n",
    "#                 image_scores     = scores[scores_sort]\n",
    "#                 image_labels     = labels[indices[scores_sort]]\n",
    "#                 image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n",
    "\n",
    "#                 # copy detections to all_detections\n",
    "#                 for label in range(dataset.num_classes()):\n",
    "#                     all_detections[index][label] = image_detections[image_detections[:, -1] == label, :-1]\n",
    "#             else:\n",
    "#                 # copy detections to all_detections\n",
    "#                 for label in range(dataset.num_classes()):\n",
    "#                     all_detections[index][label] = np.zeros((0, 5))\n",
    "\n",
    "#             print('{}/{}'.format(index + 1, len(dataset)), end='\\r')\n",
    "\n",
    "#     return all_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retinanet.load_state_dict('weights/RSNA_retinanet_3.pt')\n",
    "def collaterTest(data):\n",
    "\n",
    "    imgs = [s['img'] for s in data]\n",
    "    names = [s['name'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "        \n",
    "    widths = [int(s.shape[0]) for s in imgs]\n",
    "    heights = [int(s.shape[1]) for s in imgs]\n",
    "    batch_size = len(imgs)\n",
    "\n",
    "    max_width = np.array(widths).max()\n",
    "    max_height = np.array(heights).max()\n",
    "\n",
    "    padded_imgs = torch.zeros(batch_size, max_width, max_height, 3)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        img = imgs[i]\n",
    "        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img\n",
    "\n",
    "#    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "    \n",
    "#     if max_num_annots > 0:\n",
    "\n",
    "#         annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "#         if max_num_annots > 0:\n",
    "#             for idx, annot in enumerate(annots):\n",
    "#                 #print(annot.shape)\n",
    "#                 if annot.shape[0] > 0:\n",
    "#                     annot_padded[idx, :annot.shape[0], :] = annot\n",
    "#     else:\n",
    "#         annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "\n",
    "    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': padded_imgs, 'names': names, 'scale': scales}\n",
    "\n",
    "\n",
    "sampler_test = AspectRatioBasedSampler(dataset_test, batch_size=1, drop_last=False)\n",
    "dataloader_test = DataLoader(dataset_test, num_workers=1, collate_fn=collaterTest, batch_sampler=sampler_test)\n",
    "\n",
    "retinanet = torch.load(\"weights/RSNA_retinanet_3.pt\")\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "if use_gpu:\n",
    "    retinanet = retinanet.cuda()\n",
    "\n",
    "retinanet.eval()\n",
    "\n",
    "unnormalize = UnNormalizer()\n",
    "\n",
    "# def draw_caption(image, box, caption):\n",
    "\n",
    "#     b = np.array(box).astype(int)\n",
    "#     cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
    "#     cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c15132d6-6486-407e-b07a-2077eae209ce\n",
      "Elapsed time: 0.23003411293029785\n",
      "2badcdd6-4733-47ae-bdfc-412bb853e9a1\n",
      "Elapsed time: 0.2856779098510742\n",
      "1cf2d50f-703a-4d2b-a2b2-9ccc4594c875\n",
      "Elapsed time: 0.14617586135864258\n",
      "2975a1ea-26b6-4aef-95e8-a2f914bcb087\n",
      "Elapsed time: 0.17275691032409668\n",
      "037f16ff-46b6-45bf-b737-74072c46fab7\n",
      "Elapsed time: 0.12059855461120605\n",
      "30b596fe-fdf8-404c-9d3c-11341fd91ee5\n",
      "Elapsed time: 0.06270503997802734\n",
      "281e02d7-5a23-4cd5-96c0-923faa21434e\n",
      "Elapsed time: 0.133758544921875\n",
      "0181dc21-34be-4d57-87d9-74d91afc2458\n",
      "Elapsed time: 0.09157586097717285\n",
      "2bd3afa7-414c-431f-ad98-0d1b03f175b8\n",
      "Elapsed time: 0.0783078670501709\n",
      "124d7c34-d9f3-4c96-b20d-467f9f291926\n",
      "Elapsed time: 0.1262376308441162\n",
      "0db01d52-c8d4-490d-bf26-611a5a7616f4\n",
      "Elapsed time: 0.11808896064758301\n",
      "11286989-850c-44c4-adac-50a7d762b77f\n",
      "Elapsed time: 0.06890368461608887\n",
      "266490ca-ce52-4f5b-92ff-082dae7967c0\n",
      "Elapsed time: 0.19334006309509277\n",
      "20f38c57-8a12-493d-90ab-4c6c7f0798ba\n",
      "Elapsed time: 0.2293717861175537\n",
      "237d16b3-0c02-47e5-88a8-a7e03a74a808\n",
      "Elapsed time: 0.10566854476928711\n",
      "2dbb9a07-6add-43f3-a58b-4f607789b235\n",
      "Elapsed time: 0.10440659523010254\n",
      "24a37027-3bb6-4827-8d70-cdc74d5f40db\n",
      "Elapsed time: 0.07417798042297363\n",
      "28715b54-8198-4bac-b86c-cda1c039b7cd\n",
      "Elapsed time: 0.14073514938354492\n",
      "297be3aa-dcb5-4e28-9aed-69ac55d3dd41\n",
      "Elapsed time: 0.2194538116455078\n",
      "2b554342-6224-4d4c-822c-48a56a522c3e\n",
      "Elapsed time: 0.2547743320465088\n",
      "26aff776-ff9d-40f2-b899-6b88f1355506\n",
      "Elapsed time: 0.1380312442779541\n",
      "11d0ddf4-12c0-4998-a16c-f35cd42e2ad0\n",
      "Elapsed time: 0.1642599105834961\n",
      "c1b44588-d231-4ac1-b616-8c21d2fbca6b\n",
      "Elapsed time: 0.18025922775268555\n",
      "c127904f-d321-4d79-b02d-599b73b0a734\n",
      "Elapsed time: 0.06589818000793457\n",
      "2348e7cb-e24f-4b47-9e3a-945d1942ab5e\n",
      "Elapsed time: 0.3027932643890381\n",
      "11878fd5-58f1-4dd0-b171-7c40612befb6\n",
      "Elapsed time: 0.054013729095458984\n",
      "02ae356b-c39e-474f-908f-0af600e6b03d\n",
      "Elapsed time: 0.0577998161315918\n",
      "265ef9f1-3a21-4c9e-a8fe-740d8fae99f5\n",
      "Elapsed time: 0.09650206565856934\n",
      "1ed3b831-2452-4598-809e-69bb6f40f69e\n",
      "Elapsed time: 0.1656639575958252\n",
      "2853a995-743e-43d8-9ea4-068ffc6bf420\n",
      "Elapsed time: 0.22615480422973633\n",
      "02d0f144-bcc1-4258-bbf7-aa2bb3408ecc\n",
      "Elapsed time: 0.1807091236114502\n",
      "2e83ea2a-a296-4a5d-8553-e2a0a4b5f46b\n",
      "Elapsed time: 0.10734772682189941\n",
      "1353128c-6689-4ae7-9d97-167741984ebc\n",
      "Elapsed time: 0.14680814743041992\n",
      "2676fc9d-7ace-4896-b698-17fc68131851\n",
      "Elapsed time: 0.17504405975341797\n",
      "229b1b99-4f23-4a1b-9cac-2890d07abc96\n",
      "Elapsed time: 0.19784855842590332\n",
      "10090118-fe34-4225-bf51-62894ecb4994\n",
      "Elapsed time: 0.15552258491516113\n",
      "276a4c77-21cd-4b77-b94d-5172a2ab427f\n",
      "Elapsed time: 0.07844424247741699\n",
      "0f29db26-5ee0-4f6b-a03e-0da08fdb0a3e\n",
      "Elapsed time: 0.0863502025604248\n",
      "110dfcba-05da-425e-a691-9e17a593f69b\n",
      "Elapsed time: 0.06776595115661621\n",
      "21e696f2-9d4e-42b0-8fe6-e4e59e667476\n",
      "Elapsed time: 0.06711626052856445\n",
      "1c8c4f4f-d655-4e7a-810a-fca1d19c91c0\n",
      "Elapsed time: 0.1668224334716797\n",
      "268833a7-38e7-4c97-a4f1-db5175ea34ae\n",
      "Elapsed time: 0.13573169708251953\n",
      "019a58b2-6b0c-42d8-948d-b9fcc3d47eb8\n",
      "Elapsed time: 0.06147599220275879\n",
      "243be28d-f021-457f-b86c-95394c9be74c\n",
      "Elapsed time: 0.14129900932312012\n",
      "bfed4174-2767-496f-a013-dbcea318f9ad\n",
      "Elapsed time: 0.10764837265014648\n",
      "2927b995-6caa-41d7-9a63-a5268f6b6a23\n",
      "Elapsed time: 0.13405060768127441\n",
      "0050f8bb-36a4-4a1a-8de5-2d73154c2571\n",
      "Elapsed time: 0.06461763381958008\n",
      "04eaa6de-99d9-4a90-bd0f-3fb9f486171b\n",
      "Elapsed time: 0.18297886848449707\n",
      "2ba09a39-3935-4b61-8119-266a0fa153ec\n",
      "Elapsed time: 0.18384933471679688\n",
      "c13125e5-a27b-4110-bac3-7ed3ee2bfbb1\n",
      "Elapsed time: 0.0678105354309082\n",
      "13233462-867a-425a-9857-636482a85e07\n",
      "Elapsed time: 0.05686688423156738\n",
      "289578c9-2d3b-4571-ba3a-5b7e2b5cbd6e\n",
      "Elapsed time: 0.0778951644897461\n",
      "30384474-2ef9-4cd9-813f-4c5559c18e1a\n",
      "Elapsed time: 0.23386526107788086\n",
      "14d24e04-ff6e-458c-b26d-9c87f5789a60\n",
      "Elapsed time: 0.09339046478271484\n",
      "237ce1d8-6ff1-4cf7-afa1-be822a58276c\n",
      "Elapsed time: 0.16118168830871582\n",
      "0f9b4ff9-cafe-4c96-b0e8-6e4c282ecb02\n",
      "Elapsed time: 0.16630792617797852\n",
      "22bba58c-7d19-48f5-b71f-a743039f21de\n",
      "Elapsed time: 0.08928728103637695\n",
      "1ef50d59-05b7-4c40-a722-0096532a342f\n",
      "Elapsed time: 0.09511685371398926\n",
      "2d3442b4-85fb-42b5-a095-9b1fe2c4d371\n",
      "Elapsed time: 0.1137688159942627\n",
      "20f0b519-4336-4f74-b03d-ddc95d454bdb\n",
      "Elapsed time: 0.12174272537231445\n",
      "24a4f617-41fc-4abe-bdd7-ea28778a0f36\n",
      "Elapsed time: 0.08257555961608887\n",
      "2e5ab719-7447-4d3f-95f8-5ed5c8c736a1\n",
      "Elapsed time: 0.14057564735412598\n",
      "1328d56a-5b37-41b2-bc1b-a6c90989c642\n",
      "Elapsed time: 0.16748690605163574\n",
      "0d1d9291-e33e-4224-ab4e-c6b89f92a66c\n",
      "Elapsed time: 0.0721132755279541\n",
      "135a0985-d0f1-4c94-af72-f11e18c30bd5\n",
      "Elapsed time: 0.16441798210144043\n",
      "1c8e654a-1515-40d4-a0bd-790b24d8cd15\n",
      "Elapsed time: 0.08329105377197266\n",
      "19fb3e16-4ec8-4149-ada4-8ce5d79f41b0\n",
      "Elapsed time: 0.10536599159240723\n",
      "c07239bc-922b-420e-9f30-dede391f9fad\n",
      "Elapsed time: 0.1316390037536621\n",
      "2767048d-aef9-4a2d-be21-f8009f4f9b7e\n",
      "Elapsed time: 0.1607799530029297\n",
      "295a47cb-b808-45ee-9162-7e6e755dd064\n",
      "Elapsed time: 0.20930910110473633\n",
      "c10c617c-8be9-47c9-9972-60dcd008faea\n",
      "Elapsed time: 0.15380120277404785\n",
      "121711ca-e10e-495a-90e1-296b643f48e9\n",
      "Elapsed time: 0.12462282180786133\n",
      "2d73fa32-ca70-4084-a5eb-952988afd789\n",
      "Elapsed time: 0.15627694129943848\n",
      "29f74eff-c4f5-49d7-9ef8-fdb051100539\n",
      "Elapsed time: 0.09564018249511719\n",
      "0e85807d-4f11-43eb-9bf8-deecfcf79961\n",
      "Elapsed time: 0.061850547790527344\n",
      "2b5f9923-0ee5-42cf-b6bc-5d16ac75e9ae\n",
      "Elapsed time: 0.054560184478759766\n",
      "226d2b5a-b943-48fe-ac77-bf54f57cbca9\n",
      "Elapsed time: 0.14785528182983398\n",
      "2824b990-3060-414b-93b6-40e2ca9067ef\n",
      "Elapsed time: 0.08933401107788086\n",
      "1ff4d829-9c29-4f68-995e-2d267375c83c\n",
      "Elapsed time: 0.23184776306152344\n",
      "2289fee2-698b-46d5-aaca-00818824598a\n",
      "Elapsed time: 0.14570140838623047\n",
      "2aced5e8-cf72-4f1f-a8cf-707de0a1ddca\n",
      "Elapsed time: 0.11844277381896973\n",
      "2aed1baf-8e50-490f-abd0-2ce2d2469ea4\n",
      "Elapsed time: 0.09259653091430664\n",
      "c04e3eeb-80ec-46f1-81e4-4186ece11b3e\n",
      "Elapsed time: 0.09304261207580566\n",
      "30a5ec6a-ca1d-4125-acad-92d3474dc5bd\n",
      "Elapsed time: 0.08578157424926758\n",
      "10dd06b3-dfd5-4bfb-b2d2-184e8fc4d9be\n",
      "Elapsed time: 0.1705329418182373\n",
      "2950b52f-0d4e-4c6b-ad7e-7052b750bf9b\n",
      "Elapsed time: 0.1228477954864502\n",
      "04ba9dc3-9cf1-41f5-9e4d-939bd69c97f6\n",
      "Elapsed time: 0.08829569816589355\n",
      "2716e765-3e72-4a19-9486-ae163413958c\n",
      "Elapsed time: 0.20449566841125488\n",
      "245a6216-ef95-49ed-870e-fc415d3b96cb\n",
      "Elapsed time: 0.0569310188293457\n",
      "0340ffe8-feef-4b08-a1fa-328bb093361c\n",
      "Elapsed time: 0.09527277946472168\n",
      "12c4c9a8-99c7-46b4-a738-2fa2b7027180\n",
      "Elapsed time: 0.23482155799865723\n",
      "2d7f5392-f56e-413a-9129-fa51a6189d26\n",
      "Elapsed time: 0.08591103553771973\n",
      "1fe340ae-ec21-4b63-af7d-75ce858e126f\n",
      "Elapsed time: 0.23520302772521973\n",
      "11fc3649-c448-488a-8bf2-412ef14971d4\n",
      "Elapsed time: 0.14964556694030762\n",
      "15350598-5268-44f4-8cc8-678d1e0850d4\n",
      "Elapsed time: 0.13361215591430664\n",
      "21d231c5-d903-4acf-ab7e-b7bc7ea009f1\n",
      "Elapsed time: 0.16130995750427246\n",
      "2c14c6b4-985b-4606-b4a6-fe050f011cfc\n",
      "Elapsed time: 0.13955926895141602\n",
      "310a54ff-adc2-421e-9478-6f16c814aa20\n",
      "Elapsed time: 0.14345836639404297\n",
      "028b7794-c17e-4bef-ba97-3b9523bc1333\n",
      "Elapsed time: 0.14713740348815918\n",
      "0dbfe0c3-ec45-404f-ad6e-54683f28b52c\n",
      "Elapsed time: 0.11875677108764648\n"
     ]
    }
   ],
   "source": [
    "filepath = \"submission.csv\"\n",
    "with open(filepath, 'w') as file:\n",
    "    file.write(\"patientId,PredictionString\\n\")\n",
    "    for idx, data in enumerate(dataloader_test):\n",
    "        patientId = os.path.splitext(os.path.basename(data['names'][0]))[0]\n",
    "        print(patientId)\n",
    "        file.write(patientId+\",\")\n",
    "        with torch.no_grad():\n",
    "            st = time.time()\n",
    "            scores, classification, transformed_anchors = retinanet(data['img'].cuda().float())\n",
    "            print('Elapsed time: {}'.format(time.time()-st))\n",
    "            idxs = np.where(scores>0.5)\n",
    "            img = np.array(255 * unnormalize(data['img'][0, :, :, :])).copy()\n",
    "\n",
    "            img[img<0] = 0\n",
    "            img[img>255] = 255\n",
    "\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "#             img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            outstr = \"\"#输出到csv中\n",
    "            resize_factor = 2 #输入的是512，原图像是1024\n",
    "            for j in range(idxs[0].shape[0]):\n",
    "                bbox = transformed_anchors[idxs[0][j], :]\n",
    "                x1 = int(bbox[0])\n",
    "                y1 = int(bbox[1])\n",
    "                x2 = int(bbox[2])\n",
    "                y2 = int(bbox[3])\n",
    "                #label_name = dataset_val.labels[int(classification[idxs[0][j]])]\n",
    "#                 draw_caption(img, (x1, y1, x2, y2), \"1\")\n",
    "\n",
    "#                 cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n",
    "                #print(label_name)\n",
    "                width = x2 - x1;\n",
    "                height = y2 - y1\n",
    "                outstr += ' '\n",
    "                s = scores[idxs[0][j]].cpu().numpy()\n",
    "                print(s)\n",
    "                outstr += str(s)\n",
    "                outstr += ' '\n",
    "                bboxes_str = \"{} {} {} {}\".format(x1*resize_factor, y1*resize_factor, \\\n",
    "                                                               width*resize_factor, height*resize_factor)\n",
    "                outstr += bboxes_str\n",
    "                plt.imshow(ds.pixel_array, cmap=plt.cm.bone);\n",
    "                \n",
    "#             cv2.imshow('img', img)\n",
    "#             cv2.waitKey(0)\n",
    "            file.write(outstr+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8263395358>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
